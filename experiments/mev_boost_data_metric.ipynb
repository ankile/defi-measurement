{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import math\n",
    "\n",
    "current_path = sys.path[0]\n",
    "sys.path.append(\n",
    "    current_path[: current_path.find(\"defi-measurement\")]\n",
    "    + \"liquidity-distribution-history\"\n",
    ")\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sqlalchemy import create_engine, Column, Integer, String, Double, DateTime, Boolean, BigInteger, Float, ARRAY, CHAR\n",
    "from sqlalchemy.orm import sessionmaker, declarative_base\n",
    "\n",
    "from typing import cast\n",
    "\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "from datetime import datetime, timezone\n",
    "from preload_pool_cache import load_pool_from_blob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "postgres_uri_mp = os.environ[\"POSTGRESQL_URI_MP\"]\n",
    "postgres_uri_us = os.environ[\"POSTGRESQL_URI_US\"]\n",
    "azure_storage_uri = os.environ[\"AZURE_STORAGE_CONNECTION_STRING\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min block: 15537940, Max block: 17959956\n"
     ]
    }
   ],
   "source": [
    "minmax_block = pd.read_sql_query(\n",
    "    \"\"\"\n",
    "    SELECT\n",
    "        MIN(block_number) AS min_block,\n",
    "        MAX(block_number) AS max_block\n",
    "    FROM\n",
    "        mev_boost\n",
    "    \"\"\",\n",
    "    postgres_uri_mp,\n",
    ")\n",
    "\n",
    "min_block = minmax_block[\"min_block\"][0]\n",
    "max_block = minmax_block[\"max_block\"][0]\n",
    "\n",
    "print(f\"Min block: {min_block}, Max block: {max_block}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_swaps_for_address(address, min_block, max_block):\n",
    "    return pd.read_sql_query(\n",
    "        f\"\"\"\n",
    "        SELECT * FROM swaps\n",
    "        WHERE block_number >= {min_block}\n",
    "        AND block_number <= {max_block}\n",
    "        AND address = '{address}'\n",
    "        \"\"\",\n",
    "        postgres_uri_us,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_info = pd.read_sql_query(\n",
    "    f\"\"\"\n",
    "    SELECT * FROM token_info\n",
    "    WHERE decimals0 IS NOT NULL\n",
    "    AND decimals1 IS NOT NULL\n",
    "    \"\"\",\n",
    "    postgres_uri_us,\n",
    ").set_index(\"pool\")[[\"token0\", \"token1\", \"decimals0\", \"decimals1\"]]\n",
    "\n",
    "token_info = token_info.to_dict(orient=\"index\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate the metric for each block-pool pair\n",
    "\n",
    "The three only interesting p-norms to focus on is l_1, l_2, and infinity-norm (i.e., max).\n",
    "\n",
    "I want the following columns in the metric table:\n",
    "- block number (bigint)\n",
    "- pool address (string)\n",
    "- number of transactions in the block-pool pair (int)\n",
    "- From MEV-boost (bool) \n",
    "- MEV-boost amount (double)\n",
    "- baseline price p_0 (double) # to check correlations\n",
    "- realized order \n",
    "- realized l_1 (double)\n",
    "- realized l_2 (double)\n",
    "- realized l_infinity (double)\n",
    "- Volume heuristic l_1 (double)\n",
    "- Volume heuristic l_2 (double)\n",
    "- Volume heuristic l_infinity (double)\n",
    "\n",
    "... and add more columns for l_1, l_2, and l_infinity when new heuristics are added.\n",
    "\n",
    "Plan:\n",
    "- Find the number of pool-blocknumber pairs and split them into equal sized based on the number of cores\n",
    "- Query to get a list of pool-blocknumber pairs for each core\n",
    "- For each pool-blocknumber pair, query the transactions and calculate the metrics\n",
    "- Save the metrics to the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mev_boost_values() -> dict[int, float]:\n",
    "    res = pd.read_sql_query(\n",
    "        \"\"\"\n",
    "        SELECT block_number, mevboost_value\n",
    "        FROM\n",
    "            mev_boost\n",
    "        \"\"\",\n",
    "        postgres_uri_mp,\n",
    "    )\n",
    "    return dict(\n",
    "        zip(res.block_number, res.mevboost_value)   \n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pool_block_pairs(limit, offset) -> pd.DataFrame:\n",
    "    return pd.read_sql_query(\n",
    "        f\"\"\"\n",
    "        SELECT sc.address, sc.block_number FROM swap_counts AS sc\n",
    "        INNER JOIN token_info AS ti ON sc.address = ti.pool\n",
    "            AND ti.decimals0 IS NOT NULL AND ti.decimals1 IS NOT NULL\n",
    "        WHERE sc.block_number >= 15537940 AND sc.block_number <= 17959956\n",
    "        ORDER BY sc.address ASC, sc.block_number ASC\n",
    "        LIMIT {limit} OFFSET {offset}\n",
    "        \"\"\",\n",
    "        postgres_uri_us,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "mev_boost_values = get_mev_boost_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('0xc02aaa39b223fe8d0a0e5c4f27ead9083c756cc2', 100000000000000000)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_in = token_info[swap.address][\"token0\"] if int(swap.amount0) > 0 else token_info[swap.address][\"token1\"]\n",
    "input_amount = int(swap.amount0) if int(swap.amount0) > 0 else int(swap.amount1)\n",
    "\n",
    "token_in, input_amount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pool from Azure blob storage cache\n"
     ]
    }
   ],
   "source": [
    "pool = load_pool_from_blob(\n",
    "    swap.address,\n",
    "    postgres_uri_us,\n",
    "    azure_storage_uri,\n",
    "    \"uniswap-v3-pool-cache\",\n",
    "    verbose=True,\n",
    "    invalidate_before_date=datetime(2023, 8, 18, tzinfo=timezone.utc),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(128632716,\n",
       " Heur(totalFee=300000000000000, crossed_ticks=0, liquidity_in_range=13312430621.130419, sqrt_P=27840.164673868672, sqrtP_next=27840.172163109582, tickToFees=defaultdict(<class 'float'>, {}), inRangeTest=4.6475419889999806e+20, swapInMinusFee=9.97e+16, zeroForOne=False, gas_fee=0))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out, heur = pool.swapIn({\n",
    "    \"tokenIn\": token_in,\n",
    "    \"input\": input_amount,\n",
    "    \"as_of\": swap.block_number,\n",
    "    \"gas\": True,\n",
    "    # \"givenPrice\": curr_price,\n",
    "})\n",
    "out, heur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1290.1974130644476"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_price(heur.sqrtP_next, swap.address)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The three only interesting p-norms to focus on is l_1, l_2, and infinity-norm (i.e., max).\n",
    "\n",
    "I want the following columns in the metric table:\n",
    "- block number (bigint)\n",
    "- pool address (string)\n",
    "- number of transactions in the block-pool pair (int)\n",
    "- From MEV-boost (bool) \n",
    "- MEV-boost amount (double)\n",
    "- baseline price p_0 (double) # to check correlations\n",
    "- realized order (ARRAY(CHAR(7))) # (CONCAT(transaction_index, _, log_index))\n",
    "- realized prices (ARRAY(double))\n",
    "- realized l_1 (double)\n",
    "- realized l_2 (double)\n",
    "- realized l_infinity (double)\n",
    "- Volume heuristic order (ARRAY(CHAR(7))) # (CONCAT(transaction_index, _, log_index))\n",
    "- Volume heuristic prices (ARRAY(double))\n",
    "- Volume heuristic l_1 (double)\n",
    "- Volume heuristic l_2 (double)\n",
    "- Volume heuristic l_infinity (double)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_price(sqrt_price, pool_addr):\n",
    "    return 1 / (sqrt_price**2) / 10**(token_info[pool_addr][\"decimals0\"] - token_info[pool_addr][\"decimals1\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pool(address, it):\n",
    "    return load_pool_from_blob(\n",
    "        address,\n",
    "        postgres_uri_us,\n",
    "        azure_storage_uri,\n",
    "        \"uniswap-v3-pool-cache\",\n",
    "        verbose=False,\n",
    "        invalidate_before_date=datetime(2023, 8, 20, tzinfo=timezone.utc),\n",
    "        pbar=it,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import Boolean\n",
    "\n",
    "\n",
    "engine_mp = create_engine(postgres_uri_mp)\n",
    "\n",
    "SessionLocalMP = sessionmaker(bind=engine_mp)\n",
    "\n",
    "program_start = datetime.now()\n",
    "Base = declarative_base()\n",
    "\n",
    "class BlockMetrics(Base):\n",
    "    __tablename__ = \"block_metrics\"\n",
    "\n",
    "    # Meta Data\n",
    "    block_number = Column(Integer, primary_key=True)\n",
    "    pool_address = Column(String, primary_key=True)\n",
    "    num_transactions = Column(Integer)\n",
    "    n_buys = Column(Integer)\n",
    "    n_sells = Column(Integer)\n",
    "    baseline_price = Column(Double)\n",
    "\n",
    "    # MEV Data\n",
    "    mev_boost = Column(Boolean)\n",
    "    mev_boost_amount = Column(Double)\n",
    "\n",
    "    # Realized Data\n",
    "    realized_order = Column(ARRAY(CHAR(7)))\n",
    "    realized_prices = Column(ARRAY(Double))\n",
    "    realized_l1 = Column(Double)\n",
    "    realized_l2 = Column(Double)\n",
    "    realized_linf = Column(Double)\n",
    "\n",
    "    # Volume Heuristic Data\n",
    "    volume_heur_order = Column(ARRAY(CHAR(7)))\n",
    "    volume_heur_prices = Column(ARRAY(Double))\n",
    "    volume_heur_l1 = Column(Double)\n",
    "    volume_heur_l2 = Column(Double)\n",
    "    volume_heur_linf = Column(Double)\n",
    "    \n",
    "\n",
    "Base.metadata.create_all(engine_mp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm(prices, norm):\n",
    "    if norm == 1:\n",
    "        return float(np.sum(np.abs(prices)))\n",
    "    elif norm == 2:\n",
    "        return float(np.sqrt(np.sum(prices**2)))\n",
    "    elif norm == np.inf:\n",
    "        return float(np.max(np.abs(prices)))\n",
    "    else:\n",
    "        raise ValueError(\"Invalid norm\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_swap(swap, curr_price, pool):            \n",
    "    token_in = token_info[swap.address][\"token0\"] if int(swap.amount0) > 0 else token_info[swap.address][\"token1\"]\n",
    "    input_amount = int(swap.amount0) if int(swap.amount0) > 0 else int(swap.amount1)\n",
    "\n",
    "    _, heur = pool.swapIn({\n",
    "        \"tokenIn\": token_in,\n",
    "        \"input\": input_amount,\n",
    "        \"as_of\": swap.block_number,\n",
    "        \"gas\": True,\n",
    "        \"givenPrice\": curr_price,\n",
    "    })\n",
    "\n",
    "    return heur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pool_block_count() -> int:\n",
    "    n_pool_block_pairs = pd.read_sql_query(\n",
    "        \"\"\"\n",
    "        SELECT COUNT(*)\n",
    "        FROM swap_counts AS sc\n",
    "        INNER JOIN token_info AS ti ON sc.address = ti.pool\n",
    "            AND ti.decimals0 IS NOT NULL AND ti.decimals1 IS NOT NULL\n",
    "        WHERE sc.block_number >= 15537940 AND sc.block_number <= 17959956;\n",
    "        \"\"\",\n",
    "        postgres_uri_us,\n",
    "    ).iloc[0, 0]\n",
    "    \n",
    "    return int(n_pool_block_pairs) # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_metrics(limit, offset, process_id):\n",
    "    pool_block_pairs = get_pool_block_pairs(limit, offset)\n",
    "\n",
    "    it = tqdm(total=pool_block_pairs.shape[0])\n",
    "    pool = get_pool(pool_block_pairs.address[0], it)\n",
    "\n",
    "    program_start = datetime.now()\n",
    "\n",
    "    errors = 0\n",
    "    successes = 0\n",
    "\n",
    "    for pool_addr, df in pool_block_pairs.groupby(\"address\"):\n",
    "        it.set_description(f\"[{process_id}] ({offset}-{offset+limit}) Processing pool {pool_addr}\")\n",
    "\n",
    "        if pool_addr not in token_info:\n",
    "            print(f\"Skipping pool {pool_addr} because it is not in token_info\")\n",
    "            continue\n",
    "\n",
    "        if pool_addr != pool.pool:\n",
    "            pool = get_pool(pool_addr, it)\n",
    "\n",
    "        swaps_for_pool = get_swaps_for_address(pool_addr, df.block_number.min(), df.block_number.max())\n",
    "\n",
    "        block_numbers = df.block_number.unique()\n",
    "\n",
    "        for block_number in df.block_number.unique():\n",
    "            it.set_postfix(errors=errors, successes=successes)\n",
    "            it.update(1)\n",
    "\n",
    "            try:\n",
    "                swaps = swaps_for_pool[swaps_for_pool.block_number == block_number].sort_values(\"transaction_index\")\n",
    "\n",
    "                if swaps.shape[0] == 0:\n",
    "                    continue\n",
    "\n",
    "                curr_price = pool.getPriceAt(block_number)\n",
    "\n",
    "                swap_metric = BlockMetrics(\n",
    "                    block_number=int(block_number),\n",
    "                    pool_address=pool_addr,\n",
    "                    num_transactions=swaps.shape[0],\n",
    "                    n_buys=swaps[~swaps.amount0.str.startswith(\"-\")].shape[0],\n",
    "                    n_sells=swaps[swaps.amount0.str.startswith(\"-\")].shape[0],\n",
    "                    mev_boost=block_number in mev_boost_values,\n",
    "                    mev_boost_amount=mev_boost_values.get(block_number, 0),\n",
    "                    baseline_price=get_price(curr_price, pool_addr),\n",
    "                )\n",
    "\n",
    "                # Run the baseline measurement\n",
    "                prices = []\n",
    "                ordering = []\n",
    "                for i, (_, swap) in enumerate(swaps.iterrows()):\n",
    "\n",
    "                    heur = do_swap(swap, curr_price, pool)\n",
    "\n",
    "                    prices.append(get_price(heur.sqrtP_next, swap.address))\n",
    "                    ordering.append(f\"{swap.transaction_index:03}_{swap.log_index:03}\")\n",
    "                    curr_price = heur.sqrtP_next\n",
    "\n",
    "                swap_metric.realized_prices = prices\n",
    "                swap_metric.realized_order = ordering\n",
    "                prices_np = np.array(prices) - swap_metric.baseline_price\n",
    "                swap_metric.realized_l1 = norm(prices_np, 1)\n",
    "                swap_metric.realized_l2 = norm(prices_np, 2)\n",
    "                swap_metric.realized_linf = norm(prices_np, np.inf)\n",
    "\n",
    "                if swaps.shape[0] == 1:\n",
    "                    swap_metric.volume_heur_prices = prices\n",
    "                    swap_metric.volume_heur_order = ordering\n",
    "                    swap_metric.volume_heur_l1 = swap_metric.realized_l1\n",
    "                    swap_metric.volume_heur_l2 = swap_metric.realized_l2\n",
    "                    swap_metric.volume_heur_linf = swap_metric.realized_linf\n",
    "\n",
    "                else:\n",
    "                    # Run the volume heuristic measurement\n",
    "                    curr_price_sqrt = cast(float, pool.getPriceAt(block_number))\n",
    "                    curr_price = get_price(curr_price_sqrt, pool_addr)\n",
    "                    # baseline_price = curr_price_sqrt\n",
    "                    prices = []\n",
    "                    ordering = []\n",
    "\n",
    "                    # Split the swaps into the set of buys and sells and order by volume ascending\n",
    "                    swaps = swaps.assign(amount0_float=swaps.amount0.astype(float), amount1_float=swaps.amount1.astype(float))\n",
    "                    buy_df = swaps[~swaps.amount0.str.startswith(\"-\")]\n",
    "                    sell_df = swaps[swaps.amount0.str.startswith(\"-\")]\n",
    "                    buys = [row for _, row in buy_df.sort_values(\"amount0_float\", ascending=False).iterrows()] if buy_df.shape[0] > 0 else []\n",
    "                    sells = [row for _, row in sell_df.sort_values(\"amount1_float\", ascending=False).iterrows()] if sell_df.shape[0] > 0 else []\n",
    "\n",
    "                    # While wer're still in the core\n",
    "                    while len(buys) > 0 and len(sells) > 0:\n",
    "                        if curr_price <= swap_metric.baseline_price:\n",
    "                            swap = buys.pop(-1)\n",
    "                        else:\n",
    "                            swap = sells.pop(-1)\n",
    "\n",
    "                        heur = do_swap(swap, curr_price_sqrt, pool)\n",
    "\n",
    "                        curr_price_sqrt = heur.sqrtP_next\n",
    "                        curr_price = get_price(curr_price_sqrt, swap.address)\n",
    "                        prices.append(curr_price)\n",
    "                        ordering.append(f\"{swap.transaction_index:03}_{swap.log_index:03}\")\n",
    "\n",
    "                    # Process whatever is left in the tail\n",
    "                    for swap in (buys + sells)[::-1]:\n",
    "                        heur = do_swap(swap, curr_price_sqrt, pool)\n",
    "\n",
    "                        curr_price_sqrt = heur.sqrtP_next\n",
    "                        prices.append(get_price(curr_price_sqrt, swap.address))\n",
    "                        ordering.append(f\"{swap.transaction_index:03}_{swap.log_index:03}\")\n",
    "\n",
    "                    swap_metric.volume_heur_prices = prices\n",
    "                    swap_metric.volume_heur_order = ordering\n",
    "                    prices_np = np.array(prices) - swap_metric.baseline_price\n",
    "\n",
    "                    swap_metric.volume_heur_l1 = norm(prices_np, 1)\n",
    "                    swap_metric.volume_heur_l2 = norm(prices_np, 2)\n",
    "                    swap_metric.volume_heur_linf = norm(prices_np, np.inf)\n",
    "\n",
    "                with SessionLocalMP() as session:\n",
    "                    session.add(swap_metric)\n",
    "                    session.commit()\n",
    "                    session.close()\n",
    "\n",
    "                successes += 1\n",
    "\n",
    "            except Exception as e:\n",
    "                errors += 1\n",
    "                with open(f\"error-{program_start}.log\", \"a\") as f:\n",
    "                    f.write(f\"Error processing block {block_number} for pool {pool_addr}: {e}\\n\")\n",
    "                continue\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[6] (6681000-7794500) Processing pool 0x9e7809c21ba130c1a51c112928ea6474d9a9ae3c:   0%|          | 182/1113500 [00:11<5:38:01, 54.89it/s, errors=182, successes=0]storage cache]cache]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 16\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[39m# Create a pool of workers and map the function across the input values\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[39mwith\u001b[39;00m Pool(n_processes) \u001b[39mas\u001b[39;00m pool:\n\u001b[0;32m---> 16\u001b[0m     pool\u001b[39m.\u001b[39;49mmap(run_chunk, \u001b[39mrange\u001b[39;49m(n_processes))\n\u001b[1;32m     18\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mAll processes completed.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m/usr/lib/python3.11/multiprocessing/pool.py:367\u001b[0m, in \u001b[0;36mPool.map\u001b[0;34m(self, func, iterable, chunksize)\u001b[0m\n\u001b[1;32m    362\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmap\u001b[39m(\u001b[39mself\u001b[39m, func, iterable, chunksize\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m    363\u001b[0m \u001b[39m    \u001b[39m\u001b[39m'''\u001b[39;00m\n\u001b[1;32m    364\u001b[0m \u001b[39m    Apply `func` to each element in `iterable`, collecting the results\u001b[39;00m\n\u001b[1;32m    365\u001b[0m \u001b[39m    in a list that is returned.\u001b[39;00m\n\u001b[1;32m    366\u001b[0m \u001b[39m    '''\u001b[39;00m\n\u001b[0;32m--> 367\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_map_async(func, iterable, mapstar, chunksize)\u001b[39m.\u001b[39;49mget()\n",
      "File \u001b[0;32m/usr/lib/python3.11/multiprocessing/pool.py:768\u001b[0m, in \u001b[0;36mApplyResult.get\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    767\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget\u001b[39m(\u001b[39mself\u001b[39m, timeout\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m--> 768\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mwait(timeout)\n\u001b[1;32m    769\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mready():\n\u001b[1;32m    770\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mTimeoutError\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.11/multiprocessing/pool.py:765\u001b[0m, in \u001b[0;36mApplyResult.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    764\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwait\u001b[39m(\u001b[39mself\u001b[39m, timeout\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m--> 765\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_event\u001b[39m.\u001b[39;49mwait(timeout)\n",
      "File \u001b[0;32m/usr/lib/python3.11/threading.py:622\u001b[0m, in \u001b[0;36mEvent.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    620\u001b[0m signaled \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_flag\n\u001b[1;32m    621\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m signaled:\n\u001b[0;32m--> 622\u001b[0m     signaled \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_cond\u001b[39m.\u001b[39;49mwait(timeout)\n\u001b[1;32m    623\u001b[0m \u001b[39mreturn\u001b[39;00m signaled\n",
      "File \u001b[0;32m/usr/lib/python3.11/threading.py:320\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[39mtry\u001b[39;00m:    \u001b[39m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    319\u001b[0m     \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 320\u001b[0m         waiter\u001b[39m.\u001b[39;49macquire()\n\u001b[1;32m    321\u001b[0m         gotit \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    322\u001b[0m     \u001b[39melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from multiprocessing import Pool\n",
    "\n",
    "n_pool_block_pairs = get_pool_block_count()\n",
    "n_processes = 8\n",
    "\n",
    "# Calculate the chunk size\n",
    "chunk_size = n_pool_block_pairs // n_processes\n",
    "\n",
    "# Define a function to be mapped\n",
    "def run_chunk(i):\n",
    "    offset = i * chunk_size\n",
    "    run_metrics(chunk_size, offset, i)\n",
    "\n",
    "# Create a pool of workers and map the function across the input values\n",
    "with Pool(n_processes) as pool:\n",
    "    pool.map(run_chunk, range(n_processes))\n",
    "\n",
    "print(\"All processes completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
