{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collect data from the BigQuery database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "current_path = sys.path[0]\n",
    "sys.path.append(current_path[:current_path.find('defi-measurement')] + \"liquidity-distribution-history\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from pool_state import v3Pool\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "import json\n",
    "\n",
    "import pandas as pd\n",
    "from prisma import Prisma\n",
    "\n",
    "import psycopg2\n",
    "import psycopg2.extras\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "\n",
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "postgres_uri_us = os.getenv(\"POSTGRESQL_URI_US\")\n",
    "\n",
    "assert postgres_uri_us is not None, \"Connection string to Postgres is not set\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "pool_symbols = json.load(open(\"../addresses/pool_tokens.json\", \"r\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get data for the 10 biggest pools by TVL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "pool_addresses = [\n",
    "    \"0x88e6a0c2ddd26feeb64f039a2c41296fcb3f5640\",  # USDC-ETH  0.05%\n",
    "    \"0xcbcdf9626bc03e24f779434178a73a0b4bad62ed\",  # WBTC-ETH  0.30%\n",
    "    \"0x5777d92f208679db4b9778590fa3cab3ac9e2168\",  # DAI-USDC  0.01%\n",
    "    \"0x4585fe77225b41b697c938b018e2ac67ac5a20c0\",  # WBTC-ETC  0.05%\n",
    "    \"0xc63b0708e2f7e69cb8a1df0e1389a98c35a76d52\",  # FRAX-USDC 0.05%\n",
    "    \"0x8ad599c3a0ff1de082011efddc58f1908eb6e6d8\",  # USDC-ETH  0.30%\n",
    "    \"0x11b815efb8f581194ae79006d24e0d814b7697f6\",  # ETH-USDT  0.05%\n",
    "    \"0x3416cf6c708da44db2624d63ea0aaef7113527c6\",  # USDC-USDT 0.01%\n",
    "    \"0x7379e81228514a1d2a6cf7559203998e20598346\",  # ETH/sETH2 0.30%\n",
    "    \"0x6c6bc977e13df9b0de53b251522280bb72383700\",  # DAI-USDC  0.05%\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# it = tqdm(pool_addresses[:1])\n",
    "# for pool_address in it:\n",
    "#     it.set_description(pool_address)\n",
    "#     pool = v3Pool(pool_address, initialize=True, update=True, connStr=postgres_uri, chunk_length=5e3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New approach---Just import the whole database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_db(\n",
    "    bq_table: str,\n",
    "    pg_table: str,\n",
    "    start_index: int,\n",
    "    max_results: int,\n",
    "    total_rows: int,\n",
    ") -> None:\n",
    "    # BigQuery client\n",
    "    proj_id = \"mimetic-design-338620\"\n",
    "    bq_dataset = 'uniswap'\n",
    "\n",
    "    # Postgres connection\n",
    "    conn = psycopg2.connect(postgres_uri_us)\n",
    "    cur = conn.cursor()\n",
    "    \n",
    "    it = tqdm(total=total_rows, initial=start_index)\n",
    "\n",
    "    while True:\n",
    "        it.set_description(f\"Inserting row {start_index:_} - {start_index + max_results:_}\")\n",
    "        rows = pd.read_gbq(\n",
    "            f\"SELECT * FROM `{bq_dataset}.{bq_table}` LIMIT {max_results} OFFSET {start_index}\",\n",
    "            project_id=proj_id,\n",
    "            dialect='standard',\n",
    "            # progress_bar_type='tqdm'\n",
    "        ).to_dict('records')\n",
    "\n",
    "        if not rows:\n",
    "            break\n",
    "\n",
    "        # Insert the rows into Postgres only if row is not already present\n",
    "        psycopg2.extras.execute_values(\n",
    "            cur,\n",
    "            f\"\"\"\n",
    "            INSERT INTO {pg_table} VALUES %s\n",
    "            \"\"\",\n",
    "            # ON CONFLICT (block_number, transaction_index, log_index) DO NOTHING\n",
    "            [tuple(x.values()) for x in rows],\n",
    "            template=None,\n",
    "            page_size=100\n",
    "        )\n",
    "        conn.commit()\n",
    "\n",
    "        # Update the start index for the next batch of rows\n",
    "        start_index += max_results\n",
    "        it.update(max_results)\n",
    "\n",
    "    # Close the Postgres connection\n",
    "    cur.close()\n",
    "    conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inserting row 21_690_000 - 21_790_000:  65%|██████▍   | 21690000/33447421 [00:00<?, ?it/s]/Users/larsankile/Code/defi-measurement/venv/lib/python3.11/site-packages/google/auth/_default.py:78: UserWarning: Your application has authenticated using end user credentials from Google Cloud SDK without a quota project. You might receive a \"quota exceeded\" or \"API not enabled\" error. See the following page for troubleshooting: https://cloud.google.com/docs/authentication/adc-troubleshooting/user-creds. \n",
      "  warnings.warn(_CLOUD_SDK_CREDENTIALS_WARNING)\n",
      "Inserting row 33_490_000 - 33_590_000: : 33490000it [4:23:14, 819.83it/s]                              "
     ]
    }
   ],
   "source": [
    "# Define your BigQuery table and Postgres table\n",
    "bq_table = 'swap'\n",
    "pg_table = 'swaps'\n",
    "\n",
    "# Fetch the data from BigQuery in chunks\n",
    "start_index = 21_690_000\n",
    "max_results = 100_000  # adjust this value based on your system's memory\n",
    "total_rows = 33_447_421\n",
    "\n",
    "seed_db(bq_table, pg_table, start_index, max_results, total_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inserting row 3_000 - 103_000:   0%|          | 3000/1356519 [00:00<?, ?it/s]/Users/larsankile/Code/defi-measurement/venv/lib/python3.11/site-packages/google/auth/_default.py:78: UserWarning: Your application has authenticated using end user credentials from Google Cloud SDK without a quota project. You might receive a \"quota exceeded\" or \"API not enabled\" error. See the following page for troubleshooting: https://cloud.google.com/docs/authentication/adc-troubleshooting/user-creds. \n",
      "  warnings.warn(_CLOUD_SDK_CREDENTIALS_WARNING)\n",
      "Inserting row 1_403_000 - 1_503_000: : 1403000it [22:08, 1053.65it/s]                           \n"
     ]
    }
   ],
   "source": [
    "# Define your BigQuery table and Postgres table\n",
    "bq_table = 'MintBurnV3-labeled'\n",
    "pg_table = 'mb'\n",
    "\n",
    "# Fetch the data from BigQuery in chunks\n",
    "start_index = 3_000\n",
    "max_results = 100_000  # adjust this value based on your system's memory\n",
    "total_rows = 1_356_519\n",
    "\n",
    "seed_db(bq_table, pg_table, start_index, max_results, total_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inserting row 0 - 1_000:   0%|          | 0/13397 [00:00<?, ?it/s]/Users/larsankile/Code/defi-measurement/venv/lib/python3.11/site-packages/google/auth/_default.py:78: UserWarning: Your application has authenticated using end user credentials from Google Cloud SDK without a quota project. You might receive a \"quota exceeded\" or \"API not enabled\" error. See the following page for troubleshooting: https://cloud.google.com/docs/authentication/adc-troubleshooting/user-creds. \n",
      "  warnings.warn(_CLOUD_SDK_CREDENTIALS_WARNING)\n",
      "Inserting row 14_000 - 15_000: : 14000it [00:30, 465.99it/s]                         \n"
     ]
    }
   ],
   "source": [
    "# Define your BigQuery table and Postgres table\n",
    "bq_table = 'V3Factory_PoolCreated'\n",
    "pg_table = 'factory'\n",
    "\n",
    "# Fetch the data from BigQuery in chunks\n",
    "start_index = 0\n",
    "max_results = 1_000  # adjust this value based on your system's memory\n",
    "total_rows = 13_397\n",
    "\n",
    "seed_db(bq_table, pg_table, start_index, max_results, total_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inserting row 14_000 - 15_000: : 14000it [00:27, 507.37it/s]                         \n"
     ]
    }
   ],
   "source": [
    "# Define your BigQuery table and Postgres table\n",
    "bq_table = 'ethereum_uniswap_v3_pool_evt_initialize'\n",
    "pg_table = 'initialize'\n",
    "\n",
    "# Fetch the data from BigQuery in chunks\n",
    "start_index = 0\n",
    "max_results = 1_000  # adjust this value based on your system's memory\n",
    "total_rows = 13_360\n",
    "\n",
    "seed_db(bq_table, pg_table, start_index, max_results, total_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove the duplicate rows in the `swaps` table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "\n",
    "def remove_duplicates(pg_table, columns):\n",
    "    # establish a connection\n",
    "    conn = psycopg2.connect(postgres_uri_us)\n",
    "    conn.autocommit = False  # start a new transaction\n",
    "\n",
    "    # create a cursor\n",
    "    cur = conn.cursor()\n",
    "\n",
    "    batch_size = 100_000  # number of rows to delete in each batch\n",
    "    row_count = batch_size  # initial value to enter the loop\n",
    "\n",
    "    col_str = \", \".join(columns)  # columns as a string\n",
    "\n",
    "    # create the index if it doesn't exist\n",
    "    cur.execute(f\"\"\"\n",
    "        CREATE INDEX IF NOT EXISTS idx_swaps_columns ON {pg_table} ({col_str});\n",
    "    \"\"\")\n",
    "    conn.commit()  # commit the index creation\n",
    "\n",
    "    # loop until there's no more duplicates\n",
    "    while row_count == batch_size:\n",
    "        # find the duplicates\n",
    "        cur.execute(f\"\"\"\n",
    "            DELETE FROM {pg_table}\n",
    "            WHERE ctid IN (\n",
    "                SELECT ctid\n",
    "                FROM (\n",
    "                    SELECT ctid,\n",
    "                        ROW_NUMBER() OVER(PARTITION BY {col_str} ORDER BY ctid) AS rn\n",
    "                    FROM {pg_table}\n",
    "                ) t\n",
    "                WHERE t.rn > 1\n",
    "                LIMIT %s\n",
    "            )\n",
    "        \"\"\", (batch_size,))\n",
    "        \n",
    "        row_count = cur.rowcount  # get the number of deleted rows\n",
    "\n",
    "        # commit the deletion\n",
    "        conn.commit()\n",
    "\n",
    "        # print the progress\n",
    "        print(f\"Deleted {row_count:_} rows in this iteration\")\n",
    "\n",
    "    # close the cursor and the connection\n",
    "    cur.close()\n",
    "    conn.close()\n",
    "\n",
    "\n",
    "remove_duplicates('swaps', ['block_number', 'transaction_index', 'log_index'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted 100_000 rows in this iteration\n",
      "Deleted 100_000 rows in this iteration\n",
      "Deleted 100_000 rows in this iteration\n",
      "Deleted 100_000 rows in this iteration\n",
      "Deleted 100_000 rows in this iteration\n",
      "Deleted 100_000 rows in this iteration\n",
      "Deleted 100_000 rows in this iteration\n",
      "Deleted 100_000 rows in this iteration\n",
      "Deleted 100_000 rows in this iteration\n",
      "Deleted 23_317 rows in this iteration\n"
     ]
    }
   ],
   "source": [
    "remove_duplicates('mb', ['block_number', 'transaction_index', 'log_index'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted 13_397 rows in this iteration\n"
     ]
    }
   ],
   "source": [
    "remove_duplicates('factory', ['pool'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
