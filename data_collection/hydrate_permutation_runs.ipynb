{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prisma import Client\n",
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "import json\n",
    "import sys\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from collections import namedtuple\n",
    "\n",
    "current_path = sys.path[0]\n",
    "sys.path.append(\n",
    "    current_path[: current_path.find(\"defi-measurement\")]\n",
    "    + \"liquidity-distribution-history\"\n",
    ")\n",
    "\n",
    "from pool_state import v3Pool\n",
    "import pandera as pa\n",
    "from pandera.typing import Series, DataFrame\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "# Import Azure blob storage SDK\n",
    "from azure.storage.blob import BlobServiceClient, BlobClient, ContainerClient\n",
    "\n",
    "\n",
    "class SwapSchema(pa.DataFrameModel):\n",
    "    block_number: Series[int]\n",
    "    transaction_index: Series[int]\n",
    "    log_index: Series[int]\n",
    "    amount0: Series[str]\n",
    "    amount1: Series[str]\n",
    "    sqrtpricex96: Series[str]\n",
    "    tick: Series[int]\n",
    "    tx_hash: Series[str]\n",
    "    block_ts: Series[datetime]\n",
    "\n",
    "\n",
    "SimResult = namedtuple('SimResult', ['block_number', 'pool_address', 'n_permutations', 'data_location'])\n",
    "\n",
    "load_dotenv(override=True)\n",
    "\n",
    "postgres_uri = os.environ[\"POSTGRESQL_URI_US\"]\n",
    "azure_blob_connection_string = os.environ[\"AZURE_STORAGE_CONNECTION_STRING\"]\n",
    "\n",
    "pool_address = \"0x8ad599c3a0ff1de082011efddc58f1908eb6e6d8\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = Client()\n",
    "await db.connect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SimResult(block_number=15826457, pool_address='0x8ad599c3a0ff1de082011efddc58f1908eb6e6d8', n_permutations=1600, data_location='https://larsb973.blob.core.windows.net/uniswap-pool-pickles/permutation-simulation/simulation_15826457_1600/data.parquet')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "runs = [SimResult(row.block_number, row.pool_address, row.n_permutations, row.data_location) for row in await db.permutationsimulation.find_many(\n",
    "    where={\"n_swaps\": {\"equals\": None}}, # type: ignore\n",
    ")]\n",
    "\n",
    "runs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine\n",
    "\n",
    "\n",
    "def swaps_from_pool(pool_address: str, after: int = 0):\n",
    "    engine = create_engine(postgres_uri)\n",
    "\n",
    "    df = pd.read_sql(\n",
    "        f\"\"\"\n",
    "            SELECT block_number, transaction_index,\n",
    "                log_index, amount0, amount1,\n",
    "                sqrtpricex96,tick, tx_hash, block_ts\n",
    "            FROM swaps\n",
    "            WHERE address = '{pool_address}'\n",
    "            AND block_number >= {after};\n",
    "        \"\"\",\n",
    "        engine,\n",
    "    )\n",
    "\n",
    "    engine.dispose()\n",
    "    return df\n",
    "\n",
    "df = swaps_from_pool(pool_address, int(15e6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def run_simulation(pool: v3Pool, swaps_parameters: list, pbar=True) -> np.ndarray:\n",
    "    # Get the sqrtPriceX96 at the start of the block\n",
    "    sqrtPrice_next = pool.getPriceAt(swaps_parameters[0][\"as_of\"])\n",
    "\n",
    "    prices = np.zeros(len(swaps_parameters) + 1, dtype=np.float64)\n",
    "\n",
    "    for i, s in tqdm(enumerate(swaps_parameters, start=0), disable=not pbar):\n",
    "        s[\"givenPrice\"] = sqrtPrice_next\n",
    "        _, heur = pool.swapIn(s, fees=True)\n",
    "        sqrtPrice_next = heur.sqrtP_next\n",
    "        prices[i] = 1 / (heur.sqrt_P**2 / 1e12)\n",
    "\n",
    "    # Calculate the price at the end of the block\n",
    "    prices[-1] = 1 / (sqrtPrice_next**2 / 1e12)\n",
    "\n",
    "    return prices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_swap_params(pool: v3Pool, swaps: DataFrame[SwapSchema]) -> list:\n",
    "    # Convert the dataframe into a list of swap parameter dicts\n",
    "    swaps_parameters = []\n",
    "    for row in swaps.to_dict(orient=\"records\"):\n",
    "        # Calculate what is tokenIn and what is tokenOut\n",
    "        token_in = pool.token0\n",
    "        amount0 = str(row[\"amount0\"])\n",
    "        if str(row[\"amount0\"])[0].startswith(\"-\"):\n",
    "            token_in = pool.token1\n",
    "            amount0 = str(row[\"amount1\"])\n",
    "\n",
    "        swapParams = {\n",
    "            \"tokenIn\": token_in,\n",
    "            \"input\": int(amount0),\n",
    "            \"gasFee\": True,\n",
    "            \"as_of\": row[\"block_number\"] + 0 / 1e4,\n",
    "        }\n",
    "\n",
    "        swaps_parameters.append(swapParams)\n",
    "\n",
    "    return swaps_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pool from cache\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pickle\n",
    "\n",
    "\n",
    "def load_pool(\n",
    "    pool_address: str,\n",
    "    postgres_uri: str,\n",
    ") -> v3Pool:\n",
    "    # Check if pool_cache.pickle exists\n",
    "    filename = \"../cache/pool_cache.pickle\"\n",
    "    if not os.path.exists(filename):\n",
    "        os.mkdir(\"cache\")\n",
    "        with open(filename, \"wb\") as f:\n",
    "            pickle.dump({}, f)\n",
    "\n",
    "    # Check if we already have this pool in the cache\n",
    "    with open(filename, \"rb\") as f:\n",
    "        pool_cache = pickle.load(f)\n",
    "        if pool_address in pool_cache:\n",
    "            # If it is, load the pool from the cache\n",
    "            print(\"Loading pool from cache\")\n",
    "            return pool_cache[pool_address]\n",
    "\n",
    "    # If it's not in the cache, load it and add it to the cache\n",
    "    pool = v3Pool(\n",
    "        poolAdd=pool_address,\n",
    "        connStr=postgres_uri,\n",
    "        initialize=False,\n",
    "        delete_conn=True,\n",
    "    )\n",
    "\n",
    "    # Add the pool to the cache\n",
    "    pool_cache[pool_address] = pool\n",
    "\n",
    "    # Save the cache\n",
    "    with open(filename, \"wb\") as f:\n",
    "        pickle.dump(pool_cache, f)\n",
    "\n",
    "    return pool\n",
    "\n",
    "pool = load_pool(pool_address, postgres_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "\n",
    "\n",
    "class Stats(BaseModel):\n",
    "    baseline: float\n",
    "    original_std: float\n",
    "    permutation_stds: list\n",
    "    mean_permutation_std: float\n",
    "    permutation_abs_deviations: list\n",
    "    max_abs_permutation_deviations: list\n",
    "    mean_max_abs_permutation_deviation: float\n",
    "    abs_original_deviation: list\n",
    "    max_abs_original_deviation: float\n",
    "    permutation_areas: list\n",
    "    mean_permutation_area: float\n",
    "    original_area: float\n",
    "\n",
    "\n",
    "def calculate_stats(\n",
    "    original_prices: np.ndarray, permutation_prices: np.ndarray\n",
    ") -> Stats:\n",
    "    baseline = original_prices[0]\n",
    "    permutation_abs_deviations = np.abs(permutation_prices - baseline)\n",
    "    abs_original_deviation = np.abs(original_prices - baseline)\n",
    "\n",
    "    stats = Stats(\n",
    "        baseline=baseline.item(),\n",
    "        original_std=np.std(original_prices).item(),\n",
    "        permutation_stds=list(np.std(permutation_prices, axis=1)),\n",
    "        mean_permutation_std=np.mean(np.std(permutation_prices, axis=1)).item(),\n",
    "        permutation_abs_deviations=list(permutation_abs_deviations.tolist()),\n",
    "        max_abs_permutation_deviations=list(np.max(permutation_abs_deviations, axis=1)),\n",
    "        mean_max_abs_permutation_deviation=np.mean(\n",
    "            np.max(permutation_abs_deviations, axis=1)\n",
    "        ).item(),\n",
    "        abs_original_deviation=list(abs_original_deviation),\n",
    "        max_abs_original_deviation=np.max(abs_original_deviation).item(),\n",
    "        permutation_areas=list(np.trapz(permutation_abs_deviations, axis=1)),  # type: ignore\n",
    "        mean_permutation_area=np.mean(np.trapz(permutation_abs_deviations, axis=1)).item(),  # type: ignore\n",
    "        original_area=np.trapz(abs_original_deviation).item(),  # type: ignore\n",
    "    )\n",
    "\n",
    "    return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "13it [00:00, 18.82it/s]00:00<?, ?it/s]\n",
      "11it [00:00, 17.76it/s]00:01<01:11,  1.19s/it]\n",
      "11it [00:00, 18.74it/s]00:02<01:03,  1.07s/it]\n",
      "10it [00:00, 20.16it/s]00:03<00:58,  1.01s/it]\n",
      "10it [00:00, 17.44it/s]00:04<00:55,  1.03it/s]\n",
      "9it [00:00, 17.56it/s][00:05<00:56,  1.00s/it]\n",
      "9it [00:00, 19.49it/s][00:05<00:52,  1.05it/s]\n",
      "8it [00:00, 18.89it/s][00:06<00:48,  1.10it/s]\n",
      "8it [00:00, 17.94it/s][00:07<00:44,  1.18it/s]\n",
      "8it [00:00, 19.97it/s][00:08<00:42,  1.21it/s]\n",
      "8it [00:00, 17.84it/s] [00:08<00:40,  1.26it/s]\n",
      "8it [00:00, 17.99it/s] [00:09<00:39,  1.27it/s]\n",
      "7it [00:00, 19.83it/s] [00:10<00:38,  1.27it/s]\n",
      "7it [00:00, 15.18it/s] [00:11<00:35,  1.34it/s]\n",
      "7it [00:00, 19.39it/s] [00:11<00:35,  1.33it/s]\n",
      "7it [00:00, 18.38it/s] [00:12<00:34,  1.35it/s]\n",
      "7it [00:00, 19.37it/s] [00:13<00:32,  1.37it/s]\n",
      "7it [00:00, 20.00it/s] [00:14<00:30,  1.42it/s]\n",
      "7it [00:00, 17.90it/s] [00:14<00:29,  1.46it/s]\n",
      "7it [00:00, 20.09it/s] [00:15<00:28,  1.45it/s]\n",
      "7it [00:00, 18.57it/s] [00:16<00:28,  1.45it/s]\n",
      "7it [00:00, 19.02it/s] [00:16<00:27,  1.45it/s]\n",
      "7it [00:00, 19.05it/s] [00:17<00:26,  1.46it/s]\n",
      "7it [00:00, 19.14it/s] [00:18<00:25,  1.48it/s]\n",
      "7it [00:00, 14.81it/s] [00:18<00:25,  1.47it/s]\n",
      "7it [00:00, 20.18it/s] [00:19<00:25,  1.43it/s]\n",
      "7it [00:00, 15.80it/s] [00:20<00:23,  1.46it/s]\n",
      "7it [00:00, 17.79it/s] [00:20<00:23,  1.45it/s]\n",
      "7it [00:00, 19.30it/s] [00:21<00:22,  1.47it/s]\n",
      "7it [00:00, 19.84it/s] [00:22<00:21,  1.50it/s]\n",
      "7it [00:00, 16.90it/s] [00:22<00:20,  1.52it/s]\n",
      "7it [00:00, 16.97it/s] [00:23<00:20,  1.48it/s]\n",
      "6it [00:00, 16.51it/s] [00:24<00:19,  1.47it/s]\n",
      "6it [00:00, 19.57it/s] [00:24<00:18,  1.51it/s]\n",
      "6it [00:00, 16.54it/s] [00:25<00:17,  1.59it/s]\n",
      "6it [00:00, 20.03it/s] [00:25<00:16,  1.59it/s]\n",
      "6it [00:00, 18.76it/s] [00:26<00:15,  1.65it/s]\n",
      "6it [00:00, 19.42it/s] [00:27<00:14,  1.68it/s]\n",
      "6it [00:00, 18.43it/s] [00:27<00:13,  1.71it/s]\n",
      "6it [00:00, 18.73it/s] [00:28<00:12,  1.72it/s]\n",
      "6it [00:00, 18.06it/s] [00:28<00:12,  1.71it/s]\n",
      "6it [00:00, 15.63it/s] [00:29<00:11,  1.72it/s]\n",
      "6it [00:00, 16.38it/s] [00:30<00:11,  1.67it/s]\n",
      "6it [00:00, 17.66it/s] [00:30<00:10,  1.65it/s]\n",
      "6it [00:00, 20.07it/s] [00:31<00:10,  1.63it/s]\n",
      "6it [00:00, 17.07it/s] [00:31<00:09,  1.67it/s]\n",
      "6it [00:00, 20.04it/s] [00:32<00:09,  1.61it/s]\n",
      "6it [00:00, 19.17it/s] [00:33<00:08,  1.64it/s]\n",
      "6it [00:00, 17.63it/s] [00:33<00:07,  1.68it/s]\n",
      "6it [00:00, 16.09it/s] [00:34<00:07,  1.69it/s]\n",
      "6it [00:00, 18.12it/s] [00:34<00:06,  1.66it/s]\n",
      "6it [00:00, 18.08it/s] [00:35<00:06,  1.64it/s]\n",
      "6it [00:00, 18.83it/s] [00:36<00:05,  1.62it/s]\n",
      "6it [00:00, 16.91it/s] [00:36<00:04,  1.64it/s]\n",
      "6it [00:00, 17.18it/s] [00:40<00:11,  1.60s/it]\n",
      "6it [00:00, 18.72it/s] [00:41<00:07,  1.32s/it]\n",
      "6it [00:00, 19.26it/s] [00:41<00:05,  1.10s/it]\n",
      "6it [00:00, 16.36it/s] [00:42<00:03,  1.07it/s]\n",
      "6it [00:00, 19.64it/s] [00:43<00:02,  1.19it/s]\n",
      "6it [00:00, 17.94it/s] [00:43<00:01,  1.32it/s]\n",
      "6it [00:00, 19.43it/s] [00:44<00:00,  1.20it/s]\n",
      "100%|██████████| 61/61 [00:45<00:00,  1.34it/s]\n"
     ]
    }
   ],
   "source": [
    "from azure.storage.blob import BlobServiceClient\n",
    "import pandas as pd\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "service_client = BlobServiceClient.from_connection_string(azure_blob_connection_string)\n",
    "container_name = \"uniswap-pool-pickles\"\n",
    "\n",
    "for block_number, pool_address, n_permutations, data_location in tqdm(runs):\n",
    "    blob_name = data_location.split(\"uniswap-pool-pickles/\")[-1].strip()\n",
    "\n",
    "    blob_client = service_client.get_blob_client(container_name, blob_name)\n",
    "\n",
    "    # Retrieve data\n",
    "    data = pd.read_parquet(data_location).values\n",
    "    swaps_params = get_swap_params(pool, df[df.block_number == block_number])\n",
    "    original_prices = run_simulation(pool, swaps_params)\n",
    "\n",
    "    # Calculate stats\n",
    "    stats = calculate_stats(original_prices, data)\n",
    "\n",
    "    new_data = {\n",
    "        \"original_prices\": original_prices.tolist(),\n",
    "        \"permutation_prices\": data.tolist(),\n",
    "        \"stats\": stats.dict(),\n",
    "    }\n",
    "\n",
    "    # Save data to a temporary JSON file\n",
    "    with open(\"temp.json\", \"w\") as f:\n",
    "        json.dump(new_data, f)\n",
    "\n",
    "    filename = f\"{blob_name.rsplit('.', 1)[0]}.json\".strip()\n",
    "\n",
    "    new_blob_client = service_client.get_blob_client(container_name, filename)\n",
    "\n",
    "    with open(\"temp.json\", \"rb\") as f:\n",
    "        new_blob_client.upload_blob(f, overwrite=True)\n",
    "\n",
    "\n",
    "    # Update the database\n",
    "    await db.permutationsimulation.update_many(\n",
    "        where={\"block_number\": block_number, \"pool_address\": pool_address, \"n_permutations\": n_permutations},\n",
    "        data={\n",
    "            \"data_location\": new_blob_client.url,\n",
    "            \"n_swaps\": len(swaps_params),\n",
    "            \"original_std\": stats.original_std,\n",
    "            \"mean_permutation_std\": stats.mean_permutation_std,\n",
    "            \"original_area\": stats.original_area,\n",
    "            \"mean_permutation_area\": stats.mean_permutation_area, \n",
    "            \"max_abs_original_deviation\": stats.max_abs_original_deviation,\n",
    "            \"mean_max_abs_permutation_deviation\": stats.mean_max_abs_permutation_deviation,\n",
    "        },\n",
    "    )\n",
    "\n",
    "    # Since all updating was successful, delete the old blob\n",
    "    blob_client.delete_blob()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'PermutationSimulation' object has no attribute 'save'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[39m# Change the object and store back to database\u001b[39;00m\n\u001b[1;32m      6\u001b[0m a\u001b[39m.\u001b[39mdata_location \u001b[39m=\u001b[39m new_blob_client\u001b[39m.\u001b[39murl\n\u001b[0;32m----> 7\u001b[0m \u001b[39mawait\u001b[39;00m a\u001b[39m.\u001b[39;49msave(update_fields\u001b[39m=\u001b[39m[\u001b[39m\"\u001b[39m\u001b[39mdata_location\u001b[39m\u001b[39m\"\u001b[39m])\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'PermutationSimulation' object has no attribute 'save'"
     ]
    }
   ],
   "source": [
    "a = await db.permutationsimulation.find_first(\n",
    "    where={\"block_number\": block_number, \"pool_address\": pool_address, \"n_permutations\": n_permutations},\n",
    ")\n",
    "\n",
    "# Change the object and store back to database\n",
    "a.data_location = new_blob_client.url\n",
    "await a.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
